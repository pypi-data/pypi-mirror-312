version: '2'
name: hf-serverless
distribution_spec:
  description: Use (an external) Hugging Face Inference Endpoint for running LLM inference
  docker_image: null
  providers:
    inference:
    - remote::hf::serverless
    memory:
    - inline::faiss
    - remote::chromadb
    - remote::pgvector
    safety:
    - inline::llama-guard
    agents:
    - inline::meta-reference
    telemetry:
    - inline::meta-reference
image_type: conda
